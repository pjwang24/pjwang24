<!-- Glass Morphism Header -->
<div align="center">
  <img width="100%" src="https://capsule-render.vercel.app/api?type=transparent&fontColor=ffffff&text=Peter%20Wang&height=150&fontSize=60&desc=Hardware%20Engineer%20•%20UC%20Berkeley&descAlignY=75&descAlign=50" alt="header"/>
</div>

<div align="center">
  <a href="mailto:pjwang2324@berkeley.edu"><img src="https://img.shields.io/badge/Email-000000?style=for-the-badge&logo=gmail&logoColor=white" alt="Email"></a>
  <a href="https://linkedin.com/in/peterwang-eecs"><img src="https://img.shields.io/badge/LinkedIn-000000?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn"></a>
  <a href="https://github.com/pjwang24"><img src="https://img.shields.io/badge/GitHub-000000?style=for-the-badge&logo=github&logoColor=white" alt="GitHub"></a>
  <a href="tel:909-551-5416"><img src="https://img.shields.io/badge/Phone-000000?style=for-the-badge&logo=apple&logoColor=white" alt="Phone"></a>
</div>

<br>
<br>

<div align="center">
  <h3>Building silicon that thinks.</h3>
</div>

<br>

---

<br>

## Current

<table>
<tr>
<td width="50%" valign="top">

### ARM • Solutions Engineering Intern
*SoC Architecture Division*

Designing intelligence into Neoverse CSS architecture. Analyzing 20M+ pins across IP instances, building inference pipelines for AMBA protocol classification. 

**Impact:** 85% RAM reduction for SoC-scale analysis.

</td>
<td width="50%" valign="top">

### Berkeley SLICE Lab • Research
*AWS Trainium Architecture*

Developing 2D systolic arrays for the Zebra accelerator. Extending TeAAL modular architecture specifications to support AWS Trainium with PyTorch integration.

**Focus:** Profile-guided optimization for ML workloads.

</td>
</tr>
</table>

<br>

---

<br>

## Featured Work

<div align="center">

| | |
|---|---|
| **3-Stage RISC-V Processor** | Full RTL-to-GDS flow. Pipelined CPU with hazard resolution, synthesized on SKY130 PDK. Optimized for timing, area, and power. |
| **AWS Trainium Kernels** | Custom NKI kernels with engine-aware compute mapping. Manual memory scheduling for low-latency inference. |
| **Perceptron Branch Predictor** | Weighted history predictor achieving 13% misprediction reduction on synthetic benchmarks. |
| **Finger-Vein Recognition** | [IEEE ISPACS 2021](https://ieeexplore.ieee.org/abstract/document/9650980). NASNet with custom CUDA kernels, 36% training acceleration. |

</div>

<br>

---

<br>

## Technical

<div align="center">

**Design** • Verilog • SystemVerilog • VHDL • Chisel  
**Verification** • VCS • ModelSim • GTKWave • CocoTB  
**Physical** • Synopsys DC • Cadence Genus • Innovus • OpenROAD  
**Programming** • C/C++ • CUDA • Python • Go  
**Frameworks** • PyTorch • Polars • NumPy • Pandas  

</div>

<br>

---

<br>

## Education

**UC Berkeley** • B.S. Computer Science, Minor in Electrical Engineering • 2026  
Jim & Donna Gray Endowment Award • Leadership Scholar • IEEE Member

Relevant: Computer Architecture • Digital Design & Integrated Circuits • Microelectronic Devices • Machine Learning

<br>

---

<br>

## Previously

**IEEE** • Machine Learning Engineer • *Published research on biometric authentication*  
**Honda Research Institute** • AI/ML Intern • *EV optimization with telematics data*  
**UC Berkeley** • EECS Peer Advisor • *First advisor for 1,000+ students*

<br>

---

<br>

<div align="center">
  
*"Simplicity is the ultimate sophistication."*

</div>
